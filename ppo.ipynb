{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from pettingzoo.sisl import pursuit_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_actions=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Conv2d(3, 32, 3, padding=1)),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,ceil_mode=True),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            self._layer_init(nn.Linear(64 * 2 * 2,64)),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Linear(64,num_actions)),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def _layer_init(self,layer,std = np.sqrt(2),bias = 0):\n",
    "        torch.nn.init.orthogonal_(layer.weight,std)\n",
    "        torch.nn.init.constant_(layer.bias,bias)\n",
    "        return layer\n",
    "    def forward(self,x):\n",
    "        if len(x.shape)==3:\n",
    "            x =torch.tensor(x, dtype=torch.float).permute(2, 0, 1).unsqueeze(0) # transform shape[7,7,3] to [1,3,7,7]\n",
    "        else:\n",
    "            x =torch.tensor(x, dtype=torch.float).permute(0,3,1,2)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPO:\n",
    "    def __init__(self,lr, gamma,clip_ratio):\n",
    "        self.policy = Policy()\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(),lr=lr)\n",
    "        self.gamma =gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "\n",
    "    def update (self,rollouts):\n",
    "        obs,act,rew,logp_old,adv = rollouts\n",
    "        obs = np.array(obs)\n",
    "        returns = np.zeros_like(rew)\n",
    "        for t in reversed(range(len(rew))):\n",
    "            if t==len(rew)-1:\n",
    "                returns[t]=rew[t]\n",
    "            else:\n",
    "                returns[t]=rew[t]+self.gamma*returns[t+1]\n",
    "        values = self.policy(obs).detach().numpy()\n",
    "        adv = returns -np.sum(values,axis=1)\n",
    "\n",
    "        act = torch.tensor(act).long()\n",
    "        logp_old=torch.tensor(logp_old).float()\n",
    "        pi_old = self.policy(obs).gather(1,act.unsqueeze(-1)).squeeze(-1)\n",
    "        ratio = torch.exp(torch.log(pi_old))\n",
    "        surr1 = ratio*torch.from_numpy(adv).float()\n",
    "        surr2 = torch.clamp(ratio,1-self.clip_ratio,1+self.clip_ratio)*torch.from_numpy(adv).float()\n",
    "        loss = -torch.min(surr1,surr2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(lr =0.01,gamma=0.01,clip_ratio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "def train(env,epochs,steps_per_epoch,batch_size,lr,gamma,clip_ratio):\n",
    "    ppo = PPO(lr=lr,gamma = gamma,clip_ratio=clip_ratio)\n",
    "    ep_reward = deque(maxlen=10)\n",
    "    for epoch in range(epochs):\n",
    "        obs_buf,act_buf,rew_buf,logp_buf = [],[],[],[]\n",
    "        for _ in tqdm(range(steps_per_epoch)):\n",
    "            env.reset(seed=42)\n",
    "            obs,_,_,_,_ = env.last()\n",
    "            ep_reward.append(0)\n",
    "            for t in range(batch_size):\n",
    "                probs = ppo.policy(obs)\n",
    "                m = Categorical(probs)\n",
    "                act = m.sample()\n",
    "                logp = m.log_prob(act)\n",
    "                obs_buf.append(obs)\n",
    "                act_buf.append(act)\n",
    "                rew_buf.append(0)\n",
    "                logp_buf.append(logp)\n",
    "                env.step(act.item())\n",
    "                obs,rew,done,_,_= env.last()\n",
    "                ep_reward[-1]+=rew\n",
    "                rew_buf[-1]+=rew\n",
    "                if done:\n",
    "                    break\n",
    "            ppo.update((obs_buf,act_buf,rew_buf,logp_buf,np.zeros_like(rew_buf)))\n",
    "        print(\"Epoch: {}, Avg Reward: {:.2f}\".format(epoch,np.mean(ep_reward)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3394, 0.1336, 0.1356, 0.2565, 0.1350]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_evaders =2\n",
    "n_pursuers = 1\n",
    "n_catch =1\n",
    "max_cycles = 200\n",
    "env = pursuit_v4.env(n_evaders = n_evaders, n_pursuers = n_pursuers,max_cycles =max_cycles,n_catch=n_catch)\n",
    "env.reset(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Avg Reward: -12.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg Reward: -12.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg Reward: -12.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg Reward: -12.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg Reward: -12.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(env,epochs=5,steps_per_epoch=max_cycles//10,batch_size=128,lr=0.2,gamma=0.99,clip_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zelin/anaconda3/envs/mpc_main/lib/python3.10/site-packages/pygame/sysfont.py:223: UserWarning: 'fc-list' is missing, system fonts cannot be loaded on your platform\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    cnt+=1\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "    env.step(action)\n",
    "env.close()\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/zelin/ppo_pursuit/ppo.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zelin/ppo_pursuit/ppo.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m agent\u001b[39m.\u001b[39;49mpolicy(observation)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'policy'"
     ]
    }
   ],
   "source": [
    "agent.policy(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device=torch.device('cuda')\n",
    "device= torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0066], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1)\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = Agent().to(device)\n",
    "optimizer = optim.Adam(params = agent1.parameters(), lr=0.001,eps=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpc_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
